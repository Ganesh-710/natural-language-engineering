{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nescessary imports statements for the project lifecycle\n","metadata":{}},{"cell_type":"code","source":"!pip install contractions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud,STOPWORDS\nimport re,string,unicodedata\nfrom nltk.corpus import stopwords\nimport plotly.express as px\nfrom bs4 import BeautifulSoup\nimport contractions\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T22:53:19.514556Z","iopub.execute_input":"2022-05-11T22:53:19.514857Z","iopub.status.idle":"2022-05-11T22:53:38.327115Z","shell.execute_reply.started":"2022-05-11T22:53:19.514824Z","shell.execute_reply":"2022-05-11T22:53:38.326371Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Using torch to select device based on availability\nis_cuda = torch.cuda.is_available()\nif is_cuda:#If a GPU is available\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:      #CPU\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T22:59:31.371205Z","iopub.execute_input":"2022-05-11T22:59:31.371515Z","iopub.status.idle":"2022-05-11T22:59:31.378315Z","shell.execute_reply.started":"2022-05-11T22:59:31.371483Z","shell.execute_reply":"2022-05-11T22:59:31.377393Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"env = int(input('''\n                    Enter 0 for kaggle\n                    Enter 1 for google colab\n                    Enter 2 for local machine'''))\nbase_path = '../input/nlpimdb/IMDB Dataset.csv'\n\nif env == 0:   #Kaggle dev environment\n    base_path = '../input/nlpimdb/IMDB Dataset.csv'\nelif env == 1: #Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n#   base_path = \"/content/drive/MyDrive/imbd.csv\"\n    base_path = input('Enter your copied file path') \nelif env == 2: #Local Machine\n#   base_path = 'IMDB Dataset.csv'\n    base_path = input('Enter your copied file path') \n\n#Read data as pandas dataframe from csv\ndf = pd.read_csv(base_path)\ndf.head(5)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-11T23:00:54.252880Z","iopub.execute_input":"2022-05-11T23:00:54.253491Z","iopub.status.idle":"2022-05-11T23:00:57.862434Z","shell.execute_reply.started":"2022-05-11T23:00:54.253453Z","shell.execute_reply":"2022-05-11T23:00:57.861549Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing, Feature Engineering and Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Check for duplicates and drop if any\ndf = df.drop_duplicates(subset = ['review'], keep = 'first')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:02.263595Z","iopub.execute_input":"2022-05-11T23:01:02.263876Z","iopub.status.idle":"2022-05-11T23:01:02.443312Z","shell.execute_reply.started":"2022-05-11T23:01:02.263848Z","shell.execute_reply":"2022-05-11T23:01:02.442516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Check for missing value in the dataset. False meaing no missing value\ndf.isna().any().any()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:03.154484Z","iopub.execute_input":"2022-05-11T23:01:03.154765Z","iopub.status.idle":"2022-05-11T23:01:03.174520Z","shell.execute_reply.started":"2022-05-11T23:01:03.154731Z","shell.execute_reply":"2022-05-11T23:01:03.173770Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"eda_data = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:04.298050Z","iopub.execute_input":"2022-05-11T23:01:04.298627Z","iopub.status.idle":"2022-05-11T23:01:04.304016Z","shell.execute_reply.started":"2022-05-11T23:01:04.298588Z","shell.execute_reply":"2022-05-11T23:01:04.303244Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Splitting to train and test data","metadata":{}},{"cell_type":"markdown","source":" To avoid data lekage, We split our data train and test initiall. We split 80% for training and 20% as testing ","metadata":{}},{"cell_type":"code","source":"X,y = df['review'].values,df['sentiment'].values\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,stratify=y)\nprint(f'shape of train data is {x_train.shape}')\nprint(f'shape of test data is {x_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:06.449928Z","iopub.execute_input":"2022-05-11T23:01:06.450395Z","iopub.status.idle":"2022-05-11T23:01:06.519776Z","shell.execute_reply.started":"2022-05-11T23:01:06.450361Z","shell.execute_reply":"2022-05-11T23:01:06.518947Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysing the linguistic feature of the training set","metadata":{}},{"cell_type":"markdown","source":"As you can see from the graph that the given data is not unbalanced","metadata":{}},{"cell_type":"markdown","source":"### Tockenizing","metadata":{}},{"cell_type":"code","source":"def tockenizing(x_train,y_train,x_val,y_val):\n    wordList = []\n    stopWords = set(stopwords.words('english')) \n    for sent in x_train:\n        for word in sent.lower().split():\n            word = StringPreprocessing(word)\n            if word not in stopWords and word != '':\n                wordList.append(word)\n  \n    corpus = Counter(wordList)\n    # sorting on the basis of most common words\n    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n    # creating a dict\n    onehotDict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n    # tockenizing\n    fTrain,fTest = [],[]\n    for sent in x_train:\n            fTrain.append([onehotDict[StringPreprocessing(word)] for word in sent.lower().split() \n                                     if StringPreprocessing(word) in onehotDict.keys()])\n    for sent in x_val:\n            fTest.append([onehotDict[StringPreprocessing(word)] for word in sent.lower().split() \n                                    if StringPreprocessing(word) in onehotDict.keys()])\n            \n    encodedTrain = [1 if label =='positive' else 0 for label in y_train]  \n    encodedTest = [1 if label =='positive' else 0 for label in y_val] \n    return np.array(fTrain), np.array(encodedTrain),np.array(fTest), np.array(encodedTest),onehotDict\n\n\ndef StringPreprocessing(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n\n    return s\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:22.265159Z","iopub.execute_input":"2022-05-11T23:01:22.265458Z","iopub.status.idle":"2022-05-11T23:01:22.278402Z","shell.execute_reply.started":"2022-05-11T23:01:22.265426Z","shell.execute_reply":"2022-05-11T23:01:22.277580Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\nx_train,y_train,x_test,y_test,vocab = tockenizing(x_train,y_train,x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:01:22.851342Z","iopub.execute_input":"2022-05-11T23:01:22.851880Z","iopub.status.idle":"2022-05-11T23:03:08.507866Z","shell.execute_reply.started":"2022-05-11T23:01:22.851812Z","shell.execute_reply":"2022-05-11T23:03:08.505960Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)\ndef expand_contractions(text):\n    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n    text = contractions.fix(text)\n    return text    \n\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\ndef remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. cafÃ©\"\"\"\n    text = unidecode.unidecode(text)\n    return text\n\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n            final_text.append(i.strip().lower())\n    return \" \".join(final_text)\n\ndef preprocess(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    text = expand_contractions(text)\n    return text\n\n\neda_data['review']=eda_data['review'].apply(preprocess)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:03:08.509672Z","iopub.execute_input":"2022-05-11T23:03:08.509935Z","iopub.status.idle":"2022-05-11T23:03:29.650910Z","shell.execute_reply.started":"2022-05-11T23:03:08.509900Z","shell.execute_reply":"2022-05-11T23:03:29.650163Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# eda = df['review'].copy()\n# data_cleaned = []\n# seqlen = eda['review'].apply(lambda x: len(x.split()))\nl = int(len(eda_data) * 0.8)\n# train_fea = df[:l]\ntest_fea = eda_data[l:] #spliting to use\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:03:29.651981Z","iopub.execute_input":"2022-05-11T23:03:29.652221Z","iopub.status.idle":"2022-05-11T23:03:29.656109Z","shell.execute_reply.started":"2022-05-11T23:03:29.652189Z","shell.execute_reply":"2022-05-11T23:03:29.655393Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"eda_data.sentiment.value_counts().sort_index(ascending=True).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:03:29.658559Z","iopub.execute_input":"2022-05-11T23:03:29.659031Z","iopub.status.idle":"2022-05-11T23:03:29.875357Z","shell.execute_reply.started":"2022-05-11T23:03:29.658991Z","shell.execute_reply":"2022-05-11T23:03:29.874592Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Creating word cloud, Individualy for both positive and negative reviews using wordcloud module\ndef cloudOfWords(text):\n    wordcloud = WordCloud(width=1600, height=800).generate(text)\n    # Open a plot of the generated image.\n    plt.figure( figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()\n\nprint(\"WordCloud for postive reviews\".center(120, \" \"))   \ncloudOfWords(\" \".join(eda_data[eda_data.sentiment == 'positive'].review)  )   \nprint(\"WordCloud for negative reviews\".center(120, \" \"))   \ncloudOfWords(\" \".join(eda_data[eda_data.sentiment == 'negative'].review)  )   \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:03:29.876564Z","iopub.execute_input":"2022-05-11T23:03:29.876870Z","iopub.status.idle":"2022-05-11T23:04:15.827385Z","shell.execute_reply.started":"2022-05-11T23:03:29.876830Z","shell.execute_reply":"2022-05-11T23:04:15.826590Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=eda_data[eda_data['sentiment']=='positive']['review'].str.len()\nax1.hist(text_len,color='yellow')\nax1.set_title('Positive Reviews text')\ntext_len=eda_data[eda_data['sentiment']=='negative']['review'].str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Negative Reviews text')\nfig.suptitle('Characters in the text')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:15.828737Z","iopub.execute_input":"2022-05-11T23:04:15.829466Z","iopub.status.idle":"2022-05-11T23:04:16.226778Z","shell.execute_reply.started":"2022-05-11T23:04:15.829424Z","shell.execute_reply":"2022-05-11T23:04:16.226010Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=eda_data[eda_data['sentiment']=='positive']['review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='yellow')\nax1.set_title('Positive reviews text')\nword=eda_data[eda_data['sentiment']=='negative']['review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Negative reviews text')\nfig.suptitle('Average word length in each review')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:16.228242Z","iopub.execute_input":"2022-05-11T23:04:16.228743Z","iopub.status.idle":"2022-05-11T23:04:20.878344Z","shell.execute_reply.started":"2022-05-11T23:04:16.228699Z","shell.execute_reply":"2022-05-11T23:04:20.877689Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(eda_data.review)\ncorpus[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:20.879581Z","iopub.execute_input":"2022-05-11T23:04:20.879924Z","iopub.status.idle":"2022-05-11T23:04:21.896144Z","shell.execute_reply.started":"2022-05-11T23:04:20.879888Z","shell.execute_reply":"2022-05-11T23:04:21.895477Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:21.897534Z","iopub.execute_input":"2022-05-11T23:04:21.897792Z","iopub.status.idle":"2022-05-11T23:04:22.577963Z","shell.execute_reply.started":"2022-05-11T23:04:21.897759Z","shell.execute_reply":"2022-05-11T23:04:22.577288Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:22.580344Z","iopub.execute_input":"2022-05-11T23:04:22.580600Z","iopub.status.idle":"2022-05-11T23:04:22.586537Z","shell.execute_reply.started":"2022-05-11T23:04:22.580566Z","shell.execute_reply":"2022-05-11T23:04:22.585700Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def ngrams(most_common):\n    most_common_uni = dict(most_common)\n    temp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\n    temp[\"Common_words\"] = list(most_common_uni.keys())\n    temp[\"Count\"] = list(most_common_uni.values())\n    fig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\n    fig.show()\nngrams(get_top_text_ngrams(eda_data.review,20,1))  \nngrams(get_top_text_ngrams(eda_data.review,20,2))    \nngrams(get_top_text_ngrams(eda_data.review,20,3))    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:04:22.587917Z","iopub.execute_input":"2022-05-11T23:04:22.588594Z","iopub.status.idle":"2022-05-11T23:05:49.417920Z","shell.execute_reply.started":"2022-05-11T23:04:22.588557Z","shell.execute_reply":"2022-05-11T23:05:49.417248Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(f'Length of vocabulary is {len(vocab)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:05:49.419319Z","iopub.execute_input":"2022-05-11T23:05:49.419578Z","iopub.status.idle":"2022-05-11T23:05:49.424388Z","shell.execute_reply.started":"2022-05-11T23:05:49.419543Z","shell.execute_reply":"2022-05-11T23:05:49.423503Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Analysing review length","metadata":{}},{"cell_type":"code","source":"rev_len = [len(i) for i in x_train]\npd.Series(rev_len).hist()\nplt.show()\npd.Series(rev_len).describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:05:49.425616Z","iopub.execute_input":"2022-05-11T23:05:49.426024Z","iopub.status.idle":"2022-05-11T23:05:49.643401Z","shell.execute_reply.started":"2022-05-11T23:05:49.425983Z","shell.execute_reply":"2022-05-11T23:05:49.642803Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Padding","metadata":{}},{"cell_type":"markdown","source":"To maxLen each of the sequence will be padded","metadata":{}},{"cell_type":"code","source":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:26.242993Z","iopub.execute_input":"2022-05-11T23:16:26.243351Z","iopub.status.idle":"2022-05-11T23:16:26.250369Z","shell.execute_reply.started":"2022-05-11T23:16:26.243317Z","shell.execute_reply":"2022-05-11T23:16:26.249395Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#we have fewer  number of reviews with length > 500.\n#So we will consideronly those below it.\nx_train_pad = padding_(x_train,500)\nx_test_pad = padding_(x_test,500)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:27.160110Z","iopub.execute_input":"2022-05-11T23:16:27.160535Z","iopub.status.idle":"2022-05-11T23:16:27.824683Z","shell.execute_reply.started":"2022-05-11T23:16:27.160499Z","shell.execute_reply":"2022-05-11T23:16:27.823894Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Doing Batching and loading as tensor","metadata":{}},{"cell_type":"code","source":"# Creating Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n# DATALOADERS\nbatch_size = 50\n# Shuffling data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:27.826120Z","iopub.execute_input":"2022-05-11T23:16:27.826366Z","iopub.status.idle":"2022-05-11T23:16:27.834963Z","shell.execute_reply.started":"2022-05-11T23:16:27.826332Z","shell.execute_reply":"2022-05-11T23:16:27.834022Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# A batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:27.883085Z","iopub.execute_input":"2022-05-11T23:16:27.883320Z","iopub.status.idle":"2022-05-11T23:16:27.946625Z","shell.execute_reply.started":"2022-05-11T23:16:27.883293Z","shell.execute_reply":"2022-05-11T23:16:27.945941Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n        super(SentimentRNN,self).__init__()\n \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n \n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n    \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        #lstm\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True)\n        \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n    \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        # embeddings and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n        \n        \n        \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden\n\n              ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:28.269499Z","iopub.execute_input":"2022-05-11T23:16:28.269898Z","iopub.status.idle":"2022-05-11T23:16:28.282785Z","shell.execute_reply.started":"2022-05-11T23:16:28.269865Z","shell.execute_reply":"2022-05-11T23:16:28.281764Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab) + 1 #extra 1 for padding\nembedding_dim = 64\noutput_dim = 1\nhidden_dim = 256\nmodel = SentimentRNN(2,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\nmodel.to(device)#moving to gpu\nprint(model)#Model Summary","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:28.459151Z","iopub.execute_input":"2022-05-11T23:16:28.459369Z","iopub.status.idle":"2022-05-11T23:16:32.963216Z","shell.execute_reply.started":"2022-05-11T23:16:28.459343Z","shell.execute_reply":"2022-05-11T23:16:32.962334Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Training process","metadata":{}},{"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:32.965123Z","iopub.execute_input":"2022-05-11T23:16:32.965500Z","iopub.status.idle":"2022-05-11T23:16:32.971545Z","shell.execute_reply.started":"2022-05-11T23:16:32.965458Z","shell.execute_reply":"2022-05-11T23:16:32.970716Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nclip = 5\nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        if( (inputs.shape[0],inputs.shape[1]) != (batch_size,500)):\n            print(\"Validation - Input Shape Issue:\",inputs.shape)\n            continue\n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n            val_h = tuple([each.data for each in val_h])\n\n            inputs, labels = inputs.to(device), labels.to(device)\n            if( (inputs.shape[0],inputs.shape[1]) != (batch_size,500)):\n                print(\"Validation - Input Shape Issue:\",inputs.shape)\n                continue\n            output, val_h = model(inputs, val_h)\n            val_loss = criterion(output.squeeze(), labels.float())\n\n            val_losses.append(val_loss.item())\n            \n            accuracy = acc(output,labels)\n            val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:16:32.973178Z","iopub.execute_input":"2022-05-11T23:16:32.973789Z","iopub.status.idle":"2022-05-11T23:21:48.279698Z","shell.execute_reply.started":"2022-05-11T23:16:32.973754Z","shell.execute_reply":"2022-05-11T23:21:48.278921Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Model Summary","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n    \nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:21:48.281830Z","iopub.execute_input":"2022-05-11T23:21:48.282260Z","iopub.status.idle":"2022-05-11T23:21:48.616916Z","shell.execute_reply.started":"2022-05-11T23:21:48.282216Z","shell.execute_reply":"2022-05-11T23:21:48.616231Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Inferance","metadata":{}},{"cell_type":"code","source":"def predict_text(text):\n        word_seq = np.array([vocab[StringPreprocessing(word)] for word in text.split() \n                         if StringPreprocessing(word) in vocab.keys()])\n        word_seq = np.expand_dims(word_seq,axis=0)\n        pad =  torch.from_numpy(padding_(word_seq,500))\n        inputs = pad.to(device)\n        batch_size = 1\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(inputs, h)\n        return(output.item())","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:21:48.617970Z","iopub.execute_input":"2022-05-11T23:21:48.618344Z","iopub.status.idle":"2022-05-11T23:21:48.626364Z","shell.execute_reply.started":"2022-05-11T23:21:48.618305Z","shell.execute_reply":"2022-05-11T23:21:48.625563Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\nindex = 30\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(\"It's a sad movie but i liked it\")\nstatus = \"positive\" if pro > 0.51 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'Predicted sentiment is {status} with a probability of {pro}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:21:48.627704Z","iopub.execute_input":"2022-05-11T23:21:48.627991Z","iopub.status.idle":"2022-05-11T23:21:48.657695Z","shell.execute_reply.started":"2022-05-11T23:21:48.627955Z","shell.execute_reply":"2022-05-11T23:21:48.656998Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"\nindex = 32\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'predicted sentiment is {status} with a probability of {pro}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:21:48.659809Z","iopub.execute_input":"2022-05-11T23:21:48.660411Z","iopub.status.idle":"2022-05-11T23:21:48.679733Z","shell.execute_reply.started":"2022-05-11T23:21:48.660387Z","shell.execute_reply":"2022-05-11T23:21:48.679003Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#Running model on Test review data\ny_pred = []\nfor i in test_fea['review']:\n    pro = predict_text(i)\n    status = \"positive\" if pro > 0.5 else \"negative\"\n    pro = (1 - pro) if status == \"negative\" else pro\n    y_pred.append(status)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:21:48.680901Z","iopub.execute_input":"2022-05-11T23:21:48.681140Z","iopub.status.idle":"2022-05-11T23:23:52.187771Z","shell.execute_reply.started":"2022-05-11T23:21:48.681109Z","shell.execute_reply":"2022-05-11T23:23:52.186978Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(len(test_fea['sentiment']), len(y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:23:52.189132Z","iopub.execute_input":"2022-05-11T23:23:52.189384Z","iopub.status.idle":"2022-05-11T23:23:52.197186Z","shell.execute_reply.started":"2022-05-11T23:23:52.189351Z","shell.execute_reply":"2022-05-11T23:23:52.196337Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:23:52.200417Z","iopub.execute_input":"2022-05-11T23:23:52.201059Z","iopub.status.idle":"2022-05-11T23:23:52.205160Z","shell.execute_reply.started":"2022-05-11T23:23:52.201030Z","shell.execute_reply":"2022-05-11T23:23:52.204380Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# calculating confusion_matrix\ny_test = test_fea['sentiment']\nconfusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:23:52.206319Z","iopub.execute_input":"2022-05-11T23:23:52.207220Z","iopub.status.idle":"2022-05-11T23:23:52.277781Z","shell.execute_reply.started":"2022-05-11T23:23:52.207190Z","shell.execute_reply":"2022-05-11T23:23:52.276964Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred)) # calculating precision, recall, f1 score","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:23:52.279010Z","iopub.execute_input":"2022-05-11T23:23:52.279328Z","iopub.status.idle":"2022-05-11T23:23:52.703212Z","shell.execute_reply.started":"2022-05-11T23:23:52.279286Z","shell.execute_reply":"2022-05-11T23:23:52.702551Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print('Accuracy score : ', accuracy_score(y_test, y_pred)) # Calculating model accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-11T23:23:52.704185Z","iopub.execute_input":"2022-05-11T23:23:52.704579Z","iopub.status.idle":"2022-05-11T23:23:52.745679Z","shell.execute_reply.started":"2022-05-11T23:23:52.704545Z","shell.execute_reply":"2022-05-11T23:23:52.744967Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
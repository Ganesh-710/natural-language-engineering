{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1： Build an n gram language model based on nltk’s Brown corpus, provide\n# the code. (You can build a language model in a few lines of code using the\n# NLTK package, you can use bigram, trigram or higher order grams)\n\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.corpus import brown\nfrom nltk import re,FreqDist\nimport nltk\n\nmd_tokens = [word for word in word_tokenize(brown.raw()) if re.search('\\w',word) ]\n\nn = int(input(\"n in ngram = \"))\nn_grams = ngrams(md_tokens, n)\nprint(len(md_tokens))\n# for grams in n_grams :\n#    print(grams)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T22:31:45.844179Z","iopub.execute_input":"2022-05-11T22:31:45.844912Z","iopub.status.idle":"2022-05-11T22:32:15.094776Z","shell.execute_reply.started":"2022-05-11T22:31:45.844790Z","shell.execute_reply":"2022-05-11T22:32:15.094032Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2: After step 1, make simple predictions with the language model you\n# have built in question 1. We will start with two simple words – “I am”. Let\n# your n gram model to tell me what will be the next word, show me both\n# code and module generated results. \n\n\nfrom nltk.corpus import brown\nimport re,unicodedata,string,random,nltk,os,sys\nfrom nltk.probability import ConditionalFreqDist\n\n\ndef main():\n    text = \"\"\n    for sent in brown.sents():\n        text = text +  ' '.join(sent)    \n\n    # pre-process text\n    print(\"Filtering data...\")\n    words = filter(text)\n    print(\"Cleaning data...\")\n    words = clean(words)\n\n    # make language model\n    print(\"Building model...\")\n    model = n_gram(words)\n\n    print(\"Enter some phrase: \")\n    user_input = input()\n    predict(model, user_input)\n\n\n\"\"\"\n    Tokenize remaining words\n    and perform lemmatization\n\"\"\"\ndef clean(text):\n    tokens = nltk.word_tokenize(text)\n    wnl = nltk.stem.WordNetLemmatizer()\n\n    output = []\n    for words in tokens:\n        # lemmatize words\n        output.append(wnl.lemmatize(words))\n\n    return output\n\n\ndef filter(text):\n    # normalize text\n    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n    # replace html chars with ' '\n    text = re.sub('<.*?>', ' ', text)\n    # remove punctuation\n    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n    # only alphabets and numerics\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    # replace newline with space\n    text = re.sub(\"\\n\", \" \", text)\n    # lower case\n    text = text.lower()\n    # split and join the words\n    text = ' '.join(text.split())\n\n    return text\n\ndef n_gram(text):\n    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n    # bigrams = list(nltk.ngrams(text, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n\n    # make cdf\n    cfdist = ConditionalFreqDist()\n    for w1, w2, w3 in trigrams:\n        cfdist[(w1, w2)][w3] += 1\n\n    # transform freq to prob\n    for w1_w2 in cfdist:\n        total_count = float(sum(cfdist[w1_w2].values()))\n        for w3 in cfdist[w1_w2]:\n            cfdist[w1_w2][w3] /= total_count\n\n    return cfdist\n\n\"\"\"\n    Generate predictions \n\"\"\"\ndef predict(model, user_input):\n    user_input = filter(user_input)\n    user_input = user_input.split()\n\n    w1 = len(user_input) - 2\n    w2 = len(user_input)\n    prev_words = user_input[w1:w2]\n\n    # display prediction\n    try:\n        prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n#         print(\"Trigram model predictions: \", prediction)\n    except IndexError:\n        print('Sorry could not find a match ')\n        os._exit(1)\n        quit()\n \n    word = []\n    weight = []\n    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n        word.append(key)\n        weight.append(prob)\n    next_word = random.choices(word, weights=weight, k=1)\n    user_input.append(next_word[0])\n    print('\\n','Prediction - ',' '.join(user_input),'\\n')\n\n    ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n    if ask.lower() == 'y':\n        predict(model, str(user_input))\n    elif ask.lower() == 'n':\n        print(\"done\")\n        \n\nmain()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T22:32:15.096634Z","iopub.execute_input":"2022-05-11T22:32:15.097260Z","iopub.status.idle":"2022-05-11T22:33:02.807392Z","shell.execute_reply.started":"2022-05-11T22:32:15.097221Z","shell.execute_reply":"2022-05-11T22:33:02.806495Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re,unicodedata,string,random,nltk,os,sys\nfrom nltk.probability import ConditionalFreqDist\nfrom nltk.corpus import brown\ncount=0\ndef main():\n    sentenceCount = input('How many sentences would you like to generate?')\n    text = \"\"\n    for sent in brown.sents():\n        text = text +  ' '.join(sent)    \n\n    # pre-process text\n    print(\"Filtering data...\")\n    words = filter(text)\n    print(\"Cleaning data...\")\n    words = clean(words)\n\n    # make language model\n    print(\"Building model...\")\n    model = n_gram(words)\n    \n    for i in range(0,int(sentenceCount)):\n        re1=re2=re3=re4=re5=re6=re7=re8=re9=re10=''\n        for i in range(0,7):\n            if i==0:\n                data = predict(model, 'i am')\n                re1 = data\n            elif i==1:\n                re2 = predict(model, re1)\n                re3 = re2\n            elif i==2:\n                re3 = predict(model, re3)\n                re4 = re3\n            elif i==3:\n                re4 = predict(model, re4)\n                re5 = re4\n            elif i==4:\n                re6 = predict(model, re5)\n                re7 = re6\n            elif i==5:\n                re8 = predict(model, re7)\n            elif i==6:\n                re9 = predict(model, re8)\n            elif i==7:\n                re10 = predict(model, re9)  \n            \n        \n\n\"\"\"\n    Tokenize remaining words\n    and perform lemmatization\n\"\"\"\ndef clean(text):\n    tokens = nltk.word_tokenize(text)\n    wnl = nltk.stem.WordNetLemmatizer()\n\n    output = []\n    for words in tokens:\n        # lemmatize words\n        output.append(wnl.lemmatize(words))\n\n    return output\n\n\ndef filter(text):\n    # normalize text\n    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n    # replace html chars with ' '\n    text = re.sub('<.*?>', ' ', text)\n    # remove punctuation\n    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n    # only alphabets and numerics\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    # replace newline with space\n    text = re.sub(\"\\n\", \" \", text)\n    # lower case\n    text = text.lower()\n    # split and join the words\n    text = ' '.join(text.split())\n\n    return text\n\ndef n_gram(text):\n    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n\n    # make cdf\n    cfdist = ConditionalFreqDist()\n    for w1, w2, w3 in trigrams:\n        cfdist[(w1, w2)][w3] += 1\n\n    # transform freq to prob\n    for w1_w2 in cfdist:\n        total_count = float(sum(cfdist[w1_w2].values()))\n        for w3 in cfdist[w1_w2]:\n            cfdist[w1_w2][w3] /= total_count\n\n    return cfdist\n\n\"\"\"\n    Generate predictions \n\"\"\"\n\ndef predict(model, user_input):\n    global count\n    count = count + 1\n    user_input = filter(user_input)\n    user_input = user_input.split()\n\n    w1 = len(user_input) - 2\n    w2 = len(user_input)\n    prev_words = user_input[w1:w2]\n\n    # display prediction\n    try:\n        prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n    except IndexError:\n        print('Sorry could not find a match ')\n        os._exit(1)\n        quit()\n \n    word = []\n    weight = []\n    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n        word.append(key)\n        weight.append(prob)\n    next_word = random.choices(word, weights=weight, k=1)\n    user_input.append(next_word[0])\n\n    if count%7==0:\n        print('\\n','Prediction - ',' '.join(user_input)+'.','\\n')\n\n    return ' '.join(user_input)\n    \n\n       \n        \n\nmain()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T22:33:43.633572Z","iopub.execute_input":"2022-05-11T22:33:43.633864Z","iopub.status.idle":"2022-05-11T22:34:12.309308Z","shell.execute_reply.started":"2022-05-11T22:33:43.633835Z","shell.execute_reply":"2022-05-11T22:34:12.307864Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}